# BLINKG: A Benchmark for the Automation of Knowledge Graph Construction 

We present BLINKG, a benchmark for testing the capabilities of automatic solutions for the construction of
knowledge graphs from (semi)structured data. It provides:

- **Realistic scenarios**: A suite of progressively complex use cases, drawn from real-world data integration tasks, that challenge automatic solutions to align source fields with ontology concepts.

- **Standardized evaluation**: Clear metrics and gold-standard mappings to quantify precision, recall and overall mapping quality.

- **Extensible framework**: Standard data formats and evaluation scripts so that researchers can plug in new models, prompts or data sources in minutes.



## Benchmark Resources
We have divided the benchmark en three different scenarios, increasing their complexity. They can be found in the folder `scenarios`:

- Scenario 1:
- Scenario 2:
- Scenario 3:

## Benchmark Metrics


## Results

Results of our experimental evaluation can be found in `evaluation`. We evaluated six different LLMs: 
Deepseek, Gemini 2.5 pro, GPT-4 Omni, LLama-3.3-70B, Mixtral 8x22B and OpenAI o3. 


## Authors

- David Chaves-Fraga (main contact) - david.chaves at usc.es
- Carla Castedo


CiTIUS - University of Santiago de Compostela, July 2025 - Present
